<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <link rel=stylesheet  href="/css/custom.css"> <title>The World's First Frontier AI Regulation is Surprisingly Thoughtful</title> <body style="counter-reset: sidenote-counter;"> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/about/">About</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=the_worlds_first_frontier_ai_regulation_is_surprisingly_thoughtful ><a href="#the_worlds_first_frontier_ai_regulation_is_surprisingly_thoughtful" class=header-anchor >The World&#39;s First Frontier AI Regulation is Surprisingly Thoughtful</a></h1> <p><em>Cross-posted from the <a href="https://open.substack.com/pub/aifutures1/p/what-the-eus-code-of-practice-means">AI Futures blog</a></em></p> <p><img src="/assets/images/GeminiCodeBridge.png" alt="Eu&#39;s incomplete bridge to AGI" /></p> <p>We’ve <a href="https://blog.ai-futures.org/p/what-you-can-do-about-ai-2027">previously written</a> about what an individual can do to make the development of transformative AI less likely to end in disaster. How about an AGI company?<sup id="fnref:1"><a href="#fndef:1" class=fnref >[1]</a></sup> What steps should they take right now to prepare for crunch time? </p> <p>The first thing we’d recommend an AGI company do is to coordinate with other companies and with governments to stop the reckless race toward superintelligence. Failing that, our backup recommendation would be for an AGI company to invest in <strong>planning</strong> and <strong>transparency</strong>. </p> <p>We expect that during takeoff, leading AGI companies will have to make high-stakes decisions based on limited evidence under crazy time pressure. As depicted in <a href="https://ai-2027.com/">AI 2027</a>, the leading American AI company might have just weeks to decide whether to hand their GPUs to a possibly misaligned superhuman AI R&amp;D agent they don’t understand. Getting this decision wrong in either direction could lead to disaster. Deploy a misaligned agent, and it might sabotage the development of its vastly superhuman successor. Delay deploying an aligned agent, and you might pointlessly vaporize America’s lead over China or miss out on valuable alignment research the agent could have performed. </p> <p>Because decisions about when to deploy and when to pause will be so weighty and so rushed, AGI companies should plan as much as they can beforehand to make it more likely that they decide correctly. They should do extensive threat modelling to predict what risks their AI systems might create in the future and how they would know if the systems were creating those risks. The companies should decide before the eleventh hour what risks they are and are not willing to run. They should figure out what evidence of alignment they’d need to see in their model to feel confident putting oceans of FLOPs or a robot army at its disposal. </p> <p>AGI companies should leave these plans open to revision as they gain more evidence about the trajectory of AI development. But it’s wiser for them to make a plan now rather than improvising one from scratch after the superhuman AI R&amp;D agent is already trained. For the time being, we’re still under a veil of ignorance that prevents powerful actors from knowing what policies will benefit them in particular at crunch time. We should therefore expect them to make a more prosocial plan now than they would make later. We’re also concerned that if companies wait until too late in the game to plan for AGI, they won’t have enough time to consult with important external actors. The leading company’s executives and a small group of government overseers might just have to make a snap decision about how much existential risk it’s acceptable to run, without time to ask Congress or the public for input. The company might be locked down for security to the point where their engineers can no longer run the alignment and control plan by external experts. All of this argues in favor of planning in advance. </p> <p>Planning for takeoff also includes picking a procedure for making tough calls in the future. Companies need to think carefully about who gets to influence critical safety decisions and what incentives they face. It shouldn&#39;t all be up to the CEO or the shareholders because when AGI is imminent and the company’s valuation shoots up to a zillion, they’ll have a strong financial interest in not pausing. Someone whose incentive is to reduce risk needs to have influence over key decisions. Minimally, this could look like a designated safety officer who must be consulted before a risky deployment. Ideally, you’d implement something more robust, like <a href="https://cdn.governance.ai/Three_Lines_of_Defense_Against_Risks_From_AI.pdf">three lines of defense</a>. </p> <p>AGI companies should also be transparent to governments about their internal capabilities and security levels. This is because one AGI company on their own cannot do everything that needs to be done for takeoff to go well. We’ll need binding regulation on all American AGI companies to break the race to the bottom on safety. We’ll need to negotiate an international agreement to stop the AGI race between the US and China from <a href="https://www.rand.org/pubs/working_papers/WRA4005-1.html">escalating into war</a>. And we’ll need to coordinate scarce talent and compute to help the AGI companies tighten their security and execute successfully on their alignment and control plans. This will all ultimately require government intervention. </p> <p>That intervention is much more likely to be timely and helpful if AGI companies are transparent to officials all along. If government sees capabilities rising in real time, they can prepare to oversee takeoff by building capacity and situational awareness internally. But if AGI companies instead keep government in the dark until they develop a superhuman AI R&amp;D agent, and then give the President a midnight phone call asking for help, government’s response is unlikely to be competent and productive. It’s therefore safer for AGI companies to keep the government informed of their internal capabilities and security levels, even as the gap between internally and externally deployed capabilities grows, and the public loses visibility into frontier AI development. </p> <p>Up until now, AGI companies have made voluntary commitments on planning and transparency, but they’ve faced no legal obligation to prepare for takeoff, and they’ve only had to be as transparent to government as any random startup. This has changed recently, with the publication of the EU’s GPAI Code of Practice. We think the Code is an incremental but important step toward preparing the world for takeoff. For the first time, it imposes crisp, legally enforceable safety and transparency requirements on AGI companies. </p> <h2 id=a_brief_history_of_agi_companies_safety_commitments ><a href="#a_brief_history_of_agi_companies_safety_commitments" class=header-anchor >A brief history of AGI companies’ safety commitments</a></h2> <p>Up until mid 2023, leading AGI companies had made many informal commitments about planning for dangerous capabilities and about transparency. <a href="https://openai.com/charter/">OpenAI’s charter</a> mentioned the need for “adequate safety precautions” during “late-stage AGI development,” and their blog post on <a href="https://openai.com/index/planning-for-agi-and-beyond/">Planning for AGI and Beyond</a> called for iterative deployment on the way to AGI, “giv&#91;ing&#93; people, policymakers, and institutions time to understand what’s happening.” OpenAI also suggested that “major world governments” ought to have “insight about training runs above a certain scale.” <a href="https://ai.google/responsibility/principles/">Google’s AI Principles</a> promised that they would test their models for safety before release according to formal risk assessment frameworks. Anthropic’s <a href="https://www.anthropic.com/news/core-views-on-ai-safety">Core Views on AI Safety</a> stressed the importance of planning for the arrival of more powerful future AI, saying “it is prudent to do foundational work now to help reduce risks from advanced AI if and when much more powerful systems are developed.” And in the <a href="https://bidenwhitehouse.archives.gov/wp-content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf">White House Voluntary AI Commitments</a>, these three frontier AI companies plus Meta and Microsoft all agreed to work toward sharing “information on advances in frontier capabilities and emerging risks and threats” with the US government. On the whole, AGI companies were saying many of the right things, but without much specificity. </p> <p>Then in September 2023, Anthropic became the first AGI company to publish a frontier safety policy. Their original <a href="https://www-cdn.anthropic.com/1adf000c8f675958c2ee23805d91aaade1cd4613/responsible-scaling-policy.pdf">RSP</a> made an attempt at high-level threat modelling, identifying <a href="https://en.wikipedia.org/wiki/CBRN_defense">CBRN</a> or cyber misuse and autonomous replication as key paths by which an AI model could cause catastrophe. Anthropic then specified what dangerous capability measurements would convince them that their models posed an elevated risk of causing catastrophe and what precautions they would take if they saw that evidence. Further, Anthropic promised that by the time they developed a model that crossed their first set of dangerous capability thresholds, they would define a second level of capability thresholds and corresponding precautions. Then before crossing the second level, they would define a third level, and so on, so that at every point there’s always a plan for what to do next. The policy stressed that if at any level Anthropic was unable to meet the next level of safety and security requirements, they would refrain from training or deploying a model that passed the next dangerous capability threshold. </p> <p>In the following year, other leading AGI companies such as <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf">OpenAI</a> and <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/strengthening-our-frontier-safety-framework/frontier-safety-framework_3.pdf">Google DeepMind</a> adopted frontier safety policies of their own. No two companies’ policies are exactly alike, and all of them have undergone changes, but they display some <a href="https://metr.org/common-elements.pdf">common features</a>. As a rule, they all identify specific dangerous capabilities AI models may develop, lay down capability thresholds that would indicate elevated risk, and commit companies to taking specific safety precautions when their models exceed those thresholds. Generally, a frontier safety policy also includes conditions under which a company would stop building or deploying more powerful AI models for fear of catastrophe.</p> <p>Frontier safety policies &#40;FSPs&#41; are a great first step toward preparing for takeoff, and AGI companies should be applauded for adopting them. But that said, FSPs also suffer from some serious limitations. One is that safety policies are entirely voluntary, and not all frontier AGI companies have chosen to adopt them. For instance, xAI had no official published safety policy until late last month, and most frontier AI companies in China still don’t have safety policies.<sup id="fnref:2"><a href="#fndef:2" class=fnref >[2]</a></sup> Another important limitation is that safety policies are entirely self-enforced. Companies may promise to honor their FSPs, but they are not legally bound to do so. It’s unclear whether AGI companies will take costly actions like pausing lucrative deployments just because they promised to do so in an obscure PDF five years earlier. Even Anthropic, a company that takes its FSP relatively seriously, has already <a href="https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling">backpedalled on one of its original commitments</a> when it became inconvenient. </p> <h2 id=introducing_the_gpai_code_of_practice ><a href="#introducing_the_gpai_code_of_practice" class=header-anchor >Introducing the GPAI Code of Practice</a></h2> <p>The state of frontier AI safety changed quietly but significantly this year when the European Commission published the <a href="https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai">GPAI Code of Practice</a>. The Code is not a new law but rather a guide to help companies comply with an existing EU Law, the <a href="https://artificialintelligenceact.eu/">AI Act</a> of 2024. The Code was written by a team of thirteen <a href="https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice">independent experts</a> &#40;including <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a>&#41; with advice from industry and civil society. It tells AI companies deploying their products in Europe what steps they can take to ensure that they’re following the AI Act’s rules about copyright protection, transparency, safety, and security. In principle, an AI company could break the Code but argue successfully that they’re still following the EU AI Act. In practice, European authorities are expected to put heavy scrutiny on companies that try to demonstrate compliance with the AI Act without following the Code, so it’s in companies’ best interest to follow the Code if they want to stay right with the law. Moreover, all of the leading American AGI companies except Meta have already <a href="https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai#ecl-inpage-Signatories-of-the-AI-Pact">publicly indicated</a> that they intend to follow the Code.<sup id="fnref:3"><a href="#fndef:3" class=fnref >[3]</a></sup></p> <p>The most important part of the Code for AGI preparedness is the Safety and Security Chapter, which is supposed to apply only to frontier developers training the very riskiest models. The current definition presumptively covers every developer who trains a model with over <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">10^{25}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8141em;"></span><span class=mord >1</span><span class=mord ><span class=mord >0</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span> FLOPs of compute unless they can convince the European AI Office that their models are behind the frontier. This threshold is high enough that small startups and academics don’t need to worry about it,<sup id="fnref:4"><a href="#fndef:4" class=fnref >[4]</a></sup> but it’s still too low to single out the true frontier we’re most worried about. The chairs and vice-chairs who wrote the Code have publicly <a href="https://code-of-practice.ai/?section&#61;safety-security#chair-statement">acknowledged as much</a>, and the European Commission <a href="https://digital-strategy.ec.europa.eu/en/faqs/general-purpose-ai-models-ai-act-questions-answers">has indicated</a> that they plan to raise the compute threshold over time as the frontier advances. We think this is a wise plan since forcing trailing-edge developers to follow the Safety and Security Chapter could burden them without buying us much security. </p> <p>Even if the current threshold stays where it is, there’s important language in the Code that ensures it won’t fall too hard on smaller developers. For one, the AI Act exempts models developed purely for research purposes, so academics are in the clear. Commercial developers above the training compute threshold can still make a case to the AI Office that they are behind the frontier and shouldn&#39;t be covered. If their case is accepted, they’re exempt, and otherwise the Code still emphasizes proportionality, meaning that a developer whose best model is farther behind the frontier can get away with lighter safety and security measures. And if your model is weaker than at least one open weight model, the Code allows you to secure it as loosely as you like. Finally, enforcement of the Code doesn’t start until August 2026, so all companies that will be affected have plenty of time to prepare. </p> <p>But regardless of precisely where the threshold is placed, genuine AGI companies will have to comply with the safety and security chapter. Once they do, we think this chapter will make AGI companies substantially more prepared for takeoff and much more transparent to EU officials than they are now.</p> <p>The Code enhances AGI companies’ planning by requiring them to <strong>adopt safety and security frameworks</strong> similar to but stronger than existing FSPs in several ways. First, the Code requires companies to do more comprehensive threat modelling than any of them have done before. It says companies have to explicitly consider risks from CBRN weapons engineering, offensive cyber, harmful manipulation, and loss of control. This is a major step up since no FSP currently in force considers all four of these risk categories. AGI companies then have to write detailed scenarios and do formal risk modelling for each risk category, something no company has ever done as far as is publicly known. Such extensive threat modelling exercises will help AGI companies understand how precisely their models could cause harm, and that understanding should enable them to make more sensible and grounded plans.</p> <p>Second, the Code requires AGI companies to get every frontier model evaluated by “adequately qualified independent external evaluators” before deployment, effectively making them <strong>build and maintain relationships with external safety experts</strong>. This amounts to a kind of emergency preparedness. Companies must identify in advance who they would call for help if they needed to determine whether a model was severely dangerous, and they must practice working together with those experts. </p> <p>Third, AGI companies will have to <strong>assign responsibility</strong> for managing severe risks to specific people within their organizations. These internal risk overseers must be granted “appropriate resources” to do their job, they must have some level of independence, and they must be incentivized to correctly estimate risk. We expect it will be hard for EU regulators to tell from the outside whether AGI companies are following the spirit of this provision, just like it’s hard to tell now whether Anthropic’s <a href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy">Responsible Scaling Officer</a> is incentivized in the best way, or whether GDM’s <a href="https://deepmind.google/about/responsibility-safety/">AGI Safety Council</a> is as independent as one would like. In practice, we think AGI companies that don’t yet have safety officers will appoint them because of the Code, and any company that tries to disempower or compromise its safety team will face some healthy scrutiny from the EU. </p> <p>The Code also improves AGI companies’ transparency on several fronts. First, every time a company wants to place a new frontier model on the EU market, they have to <strong>evaluate it rigorously and send the results to the European AI Office</strong> within three weeks of deployment.<sup id="fnref:5"><a href="#fndef:5" class=fnref >[5]</a></sup> These evals need to be “at least state-of-the art” and they need to include <a href="https://www.frontiermodelforum.org/updates/issue-brief-preliminary-taxonomy-of-ai-bio-safety-evaluations/">open-ended tests</a> such as red-teaming and human uplift studies. In other words, an AGI company can’t just run a few cheap Q&amp;A benchmarks on their new model and call it a day. Also, their evaluations need to measure the new model’s <em>propensities</em> as well as its capabilities. In particular, AGI companies need to make a sincere effort to evaluate whether models are scheming or strategically undermining evaluations, eg, by sandbagging. The findings from all these evaluations must then be shared with EU officials, keeping Brussels abreast of capabilities and propensities trends at the frontier.</p> <p>Second, an AGI company must forecast when their AI models will exceed the next risk tiers in their framework and <strong>share the forecasts with the AI Office</strong>. These need to be quantitative forecasts supported by justifications, not just wild-ass guesses. Sharing these forecasts is a big deal for EU officials’ situational awareness. Almost no-one is better positioned to predict the course of AI development than the experts inside AGI companies, and those experts are about to start sharing their predictions with the EU. </p> <p>Third, AGI companies need to <strong>tell EU regulators how they’re doing on security and control</strong> every time they deploy a new frontier model publicly. The Code requires each company to set an explicit security goal, saying what types of threat actor they aim to be secure against. At minimum, companies must be secure against nonstate external threats and inside threats &#40;roughly <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html">RAND SL3</a>&#41;, though they’re encouraged to set more ambitious goals. Then a company has to implement reasonable security measures, document those measures, and explain to the EU why they’re sufficient to meet the security goal. This means that if a company is building a <a href="https://ai-2027.com/#narrative-2026-04-30:~:text&#61;The&#37;20AI&#37;20R&#37;26D&#37;20progress&#37;20multiplier&#37;3A">100x AI R&amp;D agent</a> with woefully inadequate SL2 security, the EU will know about it and can punish them for it. Notably, the Code also directs companies to guard against “&#40;self-&#41;exfiltration or sabotage carried out by models,” possibly by applying <a href="https://www.redwoodresearch.org/research/ai-control">control measures</a> to their AIs. The AI Office will get to see these measures and check whether they’re sufficient. </p> <p>Fourth, companies are required to <strong>monitor for serious incidents</strong> involving their AI models and to <strong>report these incidents</strong> promptly to authorities. This reporting requirement could make it more likely that we recognize an AI warning shot if one happens. If an AI company discovers that one of their models has self-exfiltrated, facilitated an attack on critical infrastructure, or been stolen by hackers, they must notify both the AI Office and relevant national governments within days. While authorities would obviously know about some incidents—eg, a <a href="https://en.wikipedia.org/wiki/2015_Ukraine_power_grid_hack">cyberattack knocking out power</a> to a whole region—they might have no idea <em>that an AI model was involved</em> without the company&#39;s report. And importantly, some critical incidents might go totally undetected without these reports. For instance, there&#39;s no obvious mechanism by which authorities would learn of a rogue replicating AI unless the company that developed it sounds the alarm. </p> <p>Finally, the Code also says AGI companies have to <strong>share the model spec and system prompt</strong> for every new frontier model with the AI Office. We’ve <a href="https://blog.ai-futures.org/p/make-the-prompt-public">previously argued</a> that it’s good for companies to be transparent with their specs and system prompts, so we’re pleased to see this step in that direction. </p> <p>All of this planning and transparency required by the Code is only as good as the AGI companies’ execution. What’s to stop them from writing crumby safety and security frameworks and model reports? How is the AI Office supposed to hold them to a high standard? The Code’s general approach is not to set a static, absolute standard that companies have to meet. Instead, it sets a dynamic standard by requiring companies’ FSPs, model evals, risk estimation, and elicitation techniques all to be “at least state-of-the-art.” Roughly, this means that a frontier developer’s safety practices always need to be as good as its industry peers’ practices or better. We hope that this language will create a healthy ratchet effect, where every time one AGI company improves its safety practices, the EU can force all other frontier companies to improve in the same way. </p> <h2 id=will_the_code_matter_at_crunch_time ><a href="#will_the_code_matter_at_crunch_time" class=header-anchor >Will the Code matter at crunch time?</a></h2> <p>It’s great that the Code of Practice makes AGI companies do some sensible things now, but you might wonder whether it will actually matter later in the timeline, when the stakes are higher and the EU has less leverage. The EU’s main tools for enforcing the AI Act are its power to fine and its control of the European market. Break the Act, and the European Commission can fine you up to 3&#37; of your revenue or even block you from serving your models in Europe if your breach was especially egregious. Right now, AGI companies still care about not getting fined and make lots of money selling their services to European businesses and consumers, so they have a strong incentive to play nice with the Commission. But we expect this to change as the companies get closer to AGI. As the models scale up, they’ll get vastly more expensive to serve without becoming much more performant in mundane use cases, so it will make less commercial sense to serve frontier models to the public. Also, the opportunity cost of serving a model publicly will rise when it becomes possible to accelerate AI R&amp;D by deploying the model internally instead. Both of these effects will push toward fewer frontier models released in the EU. </p> <p>No more frontier models released in Europe means no more model reports submitted to the AI Office, so most of the transparency provided by the Code of Practice goes away. The EU will also lose most of its leverage to stop AGI companies from breaking or watering down their safety policies once those companies aren’t afraid of the fines and no longer care about their access to the European market. The Commission can try to fine a company, but the maximum fine would be small for a company that’s going for broke on AGI and barely bothering to ship products or make revenue. Companies might simply refuse to pay, perhaps claiming immunity from the Code on national security grounds.<sup id="fnref:6"><a href="#fndef:6" class=fnref >[6]</a></sup> They might even pull out of the EU altogether at crunch time, leaving the European Commission with virtually no leverage left.<sup id="fnref:7"><a href="#fndef:7" class=fnref >[7]</a></sup></p> <p>Yet even if the EU is mostly powerless to enforce the Code of Practice at crunch time, some of the measures AGI companies previously put in place to comply with the Code may prove <em>sticky</em>. Companies will have no reason to throw away their threat modelling, detailed scenarios, and risk estimation just because they’re no longer bound by the Code. As they grow more afraid of their own models, the companies will be grateful that they built up competent safety teams to comply with the Code, and they’ll voluntarily turn to those teams all the more. Some of them will keep their partnerships with independent evaluators going as long as security restrictions allow them to, and maybe even pull staff from orgs like <a href="https://metr.org/">METR</a> or <a href="https://apolloresearch.ai">Apollo</a> into the Project if it becomes impossible to keep working with them externally. And the security and control measures a company implemented to achieve their security goal won’t automatically disappear the moment they stop caring about the AI Act. </p> <p>The Code’s transparency requirements also make it somewhat more likely that the EU—and maybe also the US government and the public—are aware enough to make wise decisions at crunch time. The European Commission will know what risks AGI companies find most worrying, when the companies predict those risks will arise, how robust each company’s security is, and much more. Some of this information will also have been shared with the public, since the Code tells AGI companies to publish their safety frameworks and model reports “if and insofar as necessary to assess and/or mitigate systemic risks.” And maybe most importantly, the Code ensures that AGI companies write critical documentation now so that it will be available to the US government in an emergency. If there were no Code, the companies might not bother to systematically document their security measures, control techniques, and capability forecasts, so if the US government urgently requested this information—either through legislation or through executive action—the companies would waste time scrambling to collect it. But thanks to the Code, the critical documents will already have been written for the AI Office, and they’ll be ready to go if the US requests them.</p> <h2 id=building_on_the_code ><a href="#building_on_the_code" class=header-anchor >Building on the Code</a></h2> <p>We’re pleased with the GPAI Code of Practice and consider it a win for humanity. Still, it has shortcomings, the most notable of which is that it’s not an American law. Only the AGI companies’ home governments will realistically be able to enforce regulations on them all the way through takeoff because no other government will have a sufficiently big legal stick to threaten them. All the leading AGI labs are in the US or China, so European regulation can only do so much down the stretch. </p> <p>Second, the Code doesn’t do as much as one might wish for transparency into internal deployments. For the reasons cited above &#40;and in AI 2027&#41;, we predict that AGI companies will move away from deploying their models to the public so they can allocate more GPU hours to internal automated AI R&amp;D. If such an internal deployment were going on, it could be extremely risky, but European officials probably wouldn&#39;t know anything about it since companies aren’t required to file reports for models they don’t place on the EU market.<sup id="fnref:8"><a href="#fndef:8" class=fnref >[8]</a></sup></p> <p>Third, employees within AGI companies—even those based in the EU—don’t get any new whistleblower protections under the AI Act. Since it looks plausible that the outside world will first hear about dangerous things going on inside AGI companies from whistleblowers, we’d prefer for them to be protected more extensively. </p> <p>One more shortcoming is that the Code does relatively little for public transparency. It requires an AGI company to write a safety and security framework and to share it with the AI Office, but they don’t have to publish it. Similarly, every time an AGI company releases a new AI model, they have to send a model report to the AI Office, but they are not strictly required to share the report with consumers using the model. This is far from ideal. Surely AGI companies shouldn&#39;t have to publish everything they disclose to government authorities—eg, to protect IP or state secrets—but they shouldn&#39;t be allowed to keep the public fully in the dark either. We call upon AGI companies to publish their safety and security frameworks and model reports, justifying any redactions they may have made, and we hope that future regulations will mandate them to do so. </p> <p>We would like to see the US &#40;and China&#41; pass regulations that mirror the best parts of the GPAI Code of Practice and improve upon its weak points. Several states are already considering bills that would require basic planning and transparency from AGI companies. California’s <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id&#61;202520260SB53">Senate Bill 53</a> would require large AI companies to publish FSPs, publish model cards for their publicly deployed AI models, and report serious incidents involving their models to state officials. The Bill goes beyond the GPAI Code of Practice in strengthening whistleblower protections for AGI company employees and in requiring AGI companies to share their FSPs and model documentation with the public, not just with regulators. New York’s proposed <a href="https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A">RAISE Act</a> would also make large AI companies publish safety policies and report serious incidents to authorities.</p> <p>These state bills do many of the right things, but there are limits to what state-level regulation can achieve. To ensure that the AGI companies prepare for takeoff and maintain adequate transparency with government, we&#39;ll need federal regulation along the lines of the GPAI Code. And this is just the first step. To avoid an unacceptably high chance of disaster, we’ll need government to do much more than enforcing transparency. Our next scenario and essay series will explain in detail what we want government to do—stay tuned.</p> <p><br/> <table class=fndef  id="fndef:1"> <tr> <td class=fndef-backref ><a href="#fnref:1">[1]</a> <td class=fndef-content >By “AGI company,” we mean an AI company that’s on course to be among the first to develop AGI. </table> <table class=fndef  id="fndef:2"> <tr> <td class=fndef-backref ><a href="#fnref:2">[2]</a> <td class=fndef-content >The one notable exception is Shanghai AI Laboratory, which has an <a href="https://concordia-ai.com/research/frontier-ai-risk-management-framework/">extremely detailed FSP</a>. </table> <table class=fndef  id="fndef:3"> <tr> <td class=fndef-backref ><a href="#fnref:3">[3]</a> <td class=fndef-content >xAI only pledged to follow the Code’s Safety and Security chapter, but as we’re about to explain, this is by far the most important chapter of the Code. Also note that Meta and xAI still have to comply with the EU AI Act as long as they do business in the EU. Their refusals to sign the full Code of Practice just means that they will have to demonstrate compliance with the Act by some other means. </table> <table class=fndef  id="fndef:4"> <tr> <td class=fndef-backref ><a href="#fnref:4">[4]</a> <td class=fndef-content >A <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>25</mn></msup></mrow><annotation encoding="application/x-tex">10^{25}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8141em;"></span><span class=mord >1</span><span class=mord ><span class=mord >0</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">25</span></span></span></span></span></span></span></span></span></span></span></span> FLOP training run <a href="https://heim.xyz/documents/Training-Compute-Thresholds.pdf">is estimated</a> to cost at least millions of dollars, beyond small developers’ means. </table> <table class=fndef  id="fndef:5"> <tr> <td class=fndef-backref ><a href="#fnref:5">[5]</a> <td class=fndef-content >This three week grace period is <a href="https://blog.redwoodresearch.org/p/attaching-requirements-to-model-releases">actually better for transparency</a> than requiring simultaneous model report submission. AGI companies rushing to claim SOTA and demonstrate rapid progress would pressure their safety teams to write hasty, uninformative reports if the writing process delayed deployment. The grace period instead lets companies deploy immediately while giving their safety teams three weeks to write comprehensive reports. </table> <table class=fndef  id="fndef:6"> <tr> <td class=fndef-backref ><a href="#fnref:6">[6]</a> <td class=fndef-content >The Code already has an explicit carve-out for companies to withhold parts of their model reports from the EU AI Office if national security laws require it. </table> <table class=fndef  id="fndef:7"> <tr> <td class=fndef-backref ><a href="#fnref:7">[7]</a> <td class=fndef-content >The EU currently controls <a href="https://epoch.ai/data/gpu-clusters">less than 7&#37;</a> of global AI compute, so the AGI companies don’t especially need European datacenters. They’re currently somewhat reliant on European talent, with most of the AGI companies maintaining offices in the EU. But this won’t matter much once the companies have superhuman AI R&amp;D agents. </table> <table class=fndef  id="fndef:8"> <tr> <td class=fndef-backref ><a href="#fnref:8">[8]</a> <td class=fndef-content >The AI Office <em>might</em> figure out that a company was using a secret model for internal AI R&amp;D by piecing together indirect evidence, including the developer’s own risk forecasts. </table> <br/> <em>Thank you to <a href="https://www.linkedin.com/in/thomas-larsen/">Thomas Larsen</a>, <a href="https://www.lesswrong.com/users/daniel-kokotajlo">Daniel Kokotajlo</a>, <a href="https://lcrpatell.github.io/">Liam Patell</a>, and someone else for excellent comments.</em></p> <div class=page-foot > <a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY-NC 4.0</a> Miles Kodama. Last modified: November 21, 2025. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </div> </div>