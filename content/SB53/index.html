<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <link rel=stylesheet  href="/css/custom.css"> <title>Introducing SB53.info</title> <body style="counter-reset: sidenote-counter;"> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/about/">About</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=introducing_sb53info ><a href="#introducing_sb53info" class=header-anchor >Introducing SB53.info</a></h1> <p>Today I&#39;m launching <a href="https://sb53.info/">SB53.info</a>, a resource to help people understand California&#39;s Senate Bill 53. The site shows the text of SB 53 enriched with annotations to explain its political and scientific context. </p> <p><em>Note: SB 53 went through amendments between the publication of this article and its signing into law. This article has</em> not <em>been updated to take account of the amendments. For an updated version, see <a href="https://sb53.info/summary">SB53.info</a></em></p> <h2 id=what_does_sb_53_do ><a href="#what_does_sb_53_do" class=header-anchor >What does SB 53 do?</a></h2> <p>In a nutshell, SB 53 says seven things:</p> <ul> <li><p>Every large AI developer must write, publish, and follow a safety policy &#40;§ 22757.12.a&#41;.</p> <li><p>Every year starting in 2030, a large AI developer must get an independent auditor to verify that &#40;1&#41; they are following their own safety policy, and &#40;2&#41; the safety policy is clear enough that it&#39;s possible to determine whether the developer is following it &#40;§ 22757.14&#41;. </p> <li><p>Every frontier model released by a large AI developer must have a published model card &#40;§ 22757.12.c&#41;. </p> <li><p>A large AI developer is civilly liable if they break any of these rules &#40;§&nbsp22757.16&#41;. </p> <li><p>The California Attorney General will operate an incident reporting system for critical incidents involving AI &#40;§ 22757.13&#41;.</p> <li><p>Whistleblower protections for large AI developers&#39; employees and external partners are expanded &#40;§ 1107&#41;.</p> <li><p>California will explore building a public AI compute cluster to support socially beneficial AI research and innovation &#40;§&nbsp11546.8&#41;.</p> </ul> <p>For my most thorough analysis of these key provisions, I encourage you to read <a href="https://www.sb53.info/">SB53.info</a>. Here, I&#39;ll give a high-level overview of what each provision does and how it departs from the <em>status quo</em>.</p> <p>Large AI developers are not currently mandated to adopt <strong>safety policies</strong>, but under SB 53, they would be. This wouldn&#39;t require most frontier developers to do anything qualitatively new, since they already have published safety policies, and they&#39;ve already made non-enforceable commitments to follow those policies. <a href="https://www-cdn.anthropic.com/f3b282f157017d08e36636bda1bf3bd4d9f23ee7.pdf">Anthropic</a>, <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf">OpenAI</a>, <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier&#37;20Safety&#37;20Framework&#37;202.0.pdf">Google DeepMind</a>, and <a href="https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm_source&#61;newsroom&amp;utm_medium&#61;web&amp;utm_content&#61;Frontier_AI_Framework_PDF">Meta</a> have all written safety policies that satisfy many of the requirements in § 22757.12.a, and <a href="https://x.ai/documents/2025.02.20-RMF-Draft.pdf">xAI</a> has a draft safety policy that satisfies a few of the requirements. So if SB 53 were to pass, one industry laggard would have to write a safety policy for the first time, other frontier developers would have to make their existing safety policies more robust, and every frontier developer would be legally mandated to follow their own safety policy. </p> <p>Moreover, they must get an <strong>independent auditor</strong> to certify that they are following their safety policy, and this explicitly includes certifying that the safety policy is <em>clear enough</em> for it to be determinate whether the developer is complying with it. As far as is publicly known, no major AI developer has ever undergone a safety audit, but such audits are completely routine in other risky industries <a href="https://www.ecfr.gov/current/title-14/chapter-I/subchapter-G/part-121?toc&#61;1">like aviation</a>. Every airline in the US is required to write a plan explaining what measures they will follow to ensure safety, and they must regularly commission independent audits to confirm that they&#39;re following the plan. SB 53 would require companies developing frontier AI systems to do the same. </p> <p>It is already a widely accepted best practice in the AI industry that when a company releases a new frontier model, they should publish a report called a <a href="https://arxiv.org/abs/1810.03993"><strong>model card</strong></a> describing the model&#39;s capabilities to consumers and to the scientific community. Anthropic, OpenAI, and Google DeepMind have consistently released model cards alongside all of their recent frontier models, and all three companies&#39; model cards likely comply with most of the requirements in SB 53. These cards generally explain how the developer assessed the risks posed by their model, how they intend to mitigate those risks, and whether their model reached any prespecified risk or capability thresholds. If the bill were to pass, the big three AI developers would have to disclose more detailed information about third party assessments run on their models, and developers like xAI that generally don&#39;t publish model cards would have to start publishing them. </p> <p>SB 53 would make large developers <strong>civilly liable</strong> for breaches of the above rules. No AI company executives will go to jail for failing to publish a safety policy or model card, but their companies can be faced with heavy fines—up to millions of dollars for a knowing violation of the law that causes material catastrophic risk. This is a major change from the <em>status quo</em>. Today, frontier AI developers have no legal obligation to disclose anything about their safety and security protocols to government, let alone to the public. When a company releases a new AI system more powerful than any system before, it is entirely optional under present law for them to tell consumers what dangerous things that system can do. And if a company does choose to adopt a safety policy or publish a model card, there is no force of law to guarantee the safety policy is being implemented or that the model card is accurate. This would all change under SB 53. We&#39;d no longer have to rely on AI developers&#39; good will to share critical safety information with the public. </p> <p>There is currently no official channel for the California state government to <strong>collect reports of safety incidents</strong> involving AI. If a frontier AI developer discovered tomorrow that the weights of their leading model had been stolen, the best they could do to alert state authorities would probably be to email the Attorney General&#39;s office. If a member of the public witnessed an AI autonomously causing harm in the wild, the fastest way for them to tell the authorities would probably be to tweet about it. SB 53 would replace these slow, informal information channels with an official incident reporting mechanism run by the AG. Just like California has an <a href="https://oag.ca.gov/privacy/databreach/reporting">official website to collect</a> reports of data breaches, there would be another site for reports of critical AI safety incidents. </p> <p><a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode&#61;LAB&amp;sectionNum&#61;1102.5">Existing California law</a> already offers <strong>whistleblower protection</strong> to AI company employees who report a violation of federal, state, or local law to public officials or to their superiors. Companies may not make rules or enforce contracts that would prevent their employees from blowing the whistle, nor can they retaliate against an employee who becomes a whistleblower. SB 53 expands the scope of these protections in two ways. First, it would grant whistleblower protection to actors who are currently not protected. Independent contractors, freelancers, unpaid advisors, and external groups that help developers to assess and manage catastrophic risk are not protected by existing law if they become whistleblowers, but they would be under SB 53. Second, the bill would protect disclosures of evidence that an AI developer&#39;s activities pose a catastrophic risk, whereas existing law only protects disclosures of evidence that a developer is breaking the law. Of course, many ways that a developer could cause a catastrophic risk would also involve breaking the law, but it&#39;s conceivable that a developer could do something catastrophically dangerous yet legal. It might also be easier for many would-be whistleblowers to tell whether their employer is causing a catastrophic risk than to tell whether their employer is breaking a specific law. </p> <p>Finally, SB 53 calls for California to build a <strong>publicly owned AI compute</strong> cluster called CalCompute. The cluster&#39;s purpose would be to support AI research and innovation for the public benefit. Nothing like CalCompute currently exists in California, but similar projects have been announced or are already underway in several other jurisdictions. New York has already built a compute cluster under their <a href="https://www.empireai.edu/">Empire AI</a> initiative, the UK has given academics compute access through its <a href="https://www.ukri.org/news/300-million-to-launch-first-phase-of-new-ai-research-resource">AI Research Resource</a>, and the US National Science Foundation&#39;s <a href="https://nairrpilot.org/">National AI Research Resource</a> aims to provide the same for American researchers. SB 53 does not specify how much funding California will put behind CalCompute, nor how many AI chips it aims to acquire, so it&#39;s hard to tell how much this section of the bill will accomplish. If CalCompute is funded generously in the next state budget, it could be a big deal, but if the project only gets a meager budget, it may not achieve much. </p> <h2 id=what_do_i_think_of_sb_53 ><a href="#what_do_i_think_of_sb_53" class=header-anchor >What do I think of SB 53?</a></h2> <p>I have intentionally kept <a href="https://sb53.info/">SB53.info</a> neutral. The purpose of the site is to help readers with low context on AI safety and governance get up to speed and understand SB 53, not to editorialize in favor of the bill or against it. But of course I do have an opinion on SB 53. I think it would probably be good for California and for the world if the bill passes.</p> <p>Requiring every frontier AI company to follow a safety policy and to publish model cards alongside their strongest models is just about the least we can ask them to do for safety. In the first place, we know it&#39;s not too hard for the companies to comply since Anthropic, OpenAI, and GDM already have safety policies and model cards that satisfy <em>most</em> of the requirements in SB 53. It&#39;s not asking much for them to be somewhat more transparent and for less responsible companies near the frontier to get serious about safety and transparency. </p> <p>And if we look beyond the AI industry, it becomes even clearer how light SB 53&#39;s safety and disclosure requirements are in absolute terms. We make airlines write detailed safety plans, and we hold them legally accountable if they fail to follow those plans. Why shouldn&#39;t we make the largest AI companies do the same? Would it really make sense for companies that are trying to build godlike superintelligence to do less safety planning than companies that fly planes? Again, if a company wants to sell children&#39;s crayons, we make them test those crayons for safety hazards and <a href="https://www.cpsc.gov/Business--Manufacturing/Testing-Certification/Childrens-Product-Certificate">publish a report</a> explaining why they believe it is safe to sell their crayons to consumers. Would it really be sensible for a company to deploy AGI to the public with less safety information than Crayola provides when it sells crayons?<sup id="fnref:prop"><a href="#fndef:prop" class=fnref >[1]</a></sup> My point is that the safety and transparency requirements SB 53 would impose on large AI developers are not out of the ordinary. We already impose them on just about any business that creates even a small risk to the public. </p> <p>On the other hand, SB 53 is just about the strongest AI safety regulation that we can know right now will be robustly good. In most other risky industries, we pretty much understand what steps businesses need to take to prevent accidents and keep the public safe, so we pass detailed regulations requiring them to take those steps. If you&#39;re cooking food, you need to store it at such and such a temperature, protect it from contaminants, and so on. If you&#39;re driving a truck, you have to rest for so many hours in each twenty-four hour window, and you have to check regularly that your truck is in good repair. But we are not yet in a position to pass such detailed safety regulations for AI developers. Our best threat modeling is still so janky and the sciences of alignment and control are still so immature that nobody knows precisely what you have to do to guarantee that an advanced AI will be safe. There is no checklist yet.</p> <p>The best we can do in this situation is to ask large AI developers for more transparency. As the <a href="https://www.cafrontieraigov.org/">California Report</a> repeatedly stresses, the key benefit of transparency is that it aligns the AI developers&#39; incentives with public welfare. We may not know now exactly what safety measures we&#39;ll want developers to be taking in three years,<sup id="fnref:uncertainty"><a href="#fndef:uncertainty" class=fnref >[2]</a></sup> but we&#39;ll definitely want to know then what measures they actually are taking. That way, we can assess whether their safety practices are adequate in light of all the evidence we don&#39;t have yet but will have gained in the future. We can determine what practices are standard across the AI industry and create a race to the top on safety by applauding leaders and pressuring laggards to improve. And if needed, we can write more detailed, prescriptive regulation from a position of greater knowledge than we have now. None of this is possible without basic transparency.</p> <p>To their credit, many of our leading AI developers are already quite transparent. They&#39;ve chosen to share far more information about their safety protocols and their cutting edge models than they have to for purely commercial reasons. This is great, but we shouldn&#39;t be relying on AI companies&#39; good will to keep the public informed of critical safety information. It&#39;s time to make transparency non-optional.</p> <p><table class=fndef  id="fndef:prop"> <tr> <td class=fndef-backref ><a href="#fnref:prop">[1]</a> <td class=fndef-content >The level of safety regulation we place on an industry should be proportional to the severity of the risks it poses to society. Since advanced general-purpose AI clearly poses more risk than children&#39;s crayons, the level of safety regulation we place on Crayola is a lower bound on the level of safety regulation we should place on leading AI developers. </table> <table class=fndef  id="fndef:uncertainty"> <tr> <td class=fndef-backref ><a href="#fnref:uncertainty">[2]</a> <td class=fndef-content >Why should we be uncertain about what safety measures will be appropriate in three years? What might have happened by then to invalidate our current best guesses? &#40;1&#41;&nbspThe way we train and deploy frontier AI might have changed dramatically by then. Eg, we might switch from discrete deployments to continuous learning. &#40;2&#41;&nbspThe background risk environment may have changed. Eg, we may have imposed universal nucleic acid <a href="https://www.rand.org/pubs/research_reports/RRA3329-1.html">synthesis screening</a>, reducing the need for stringent bio-misuse mitigations. &#40;3&#41;&nbspAlignment and control techniques that aren&#39;t feasible now might have become feasible, or the inverse. Eg, chain-of-thought monitoring looks like an effective strategy for controlling AIs now, but it <a href="https://tomekkorbak.com/cot-monitorability-is-a-fragile-opportunity/cot_monitoring.pdf">could soon become useless</a> if companies train their models to reason less legibly. </table> <br/> <em>Thank you to <a href="https://metr.org/team/michael-chen/">Michael Chen</a> and <a href="https://www.thomaswoodside.com/">Thomas Woodside</a> for advice and feedback on this project. All political opinions and all errors are my own.</em></p> <div class=page-foot > <a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY-NC 4.0</a> Miles Kodama. Last modified: December 26, 2025. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </div> </div>