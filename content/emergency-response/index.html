<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <link rel=stylesheet  href="/css/custom.css"> <title>Emergency Response Measures for Catastrophic AI Risk</title> <body style="counter-reset: sidenote-counter;"> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/about/">About</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=emergency_response_measures_for_catastrophic_ai_risk ><a href="#emergency_response_measures_for_catastrophic_ai_risk" class=header-anchor >Emergency Response Measures for Catastrophic AI Risk</a></h1> <p>I have written a paper on Chinese domestic AI regulation with coauthors James Zhang, Zongze Wu, Michael Chen, Yue Zhu, and Geng Hong. It was presented recently at NeurIPS 2025&#39;s <a href="https://regulatableml.github.io/">Workshop on Regulatable ML</a>, and it may be found on <a href="https://arxiv.org/pdf/2511.05526">ArXiv</a> and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id&#61;5687563">SSRN</a>.</p> <p>Here I&#39;ll explain what I take to be the key ideas of the paper in a more casual style. <em>I am speaking only for myself in this post, and</em> not <em>for any of my coauthors.</em></p> <p><img src="/assets/images/EmergencyPoster.jpeg" alt="A poster summarizing the paper" /><sup id="fnref:postercred"><a href="#fndef:postercred" class=fnref >[1]</a></sup></p> <p>The top US AI companies have better capabilities than the top Chinese companies have for now, but the US lead isn&#39;t more than a year at most, and I expect it to narrow over the next couple years.<sup id="fnref:B200s"><a href="#fndef:B200s" class=fnref >[2]</a></sup> I am therefore nearly as worried about catastrophic risk from Chinese-developed AI as I am worried about catastrophic risk from American AI. </p> <p>I would worry somewhat less if Chinese AI companies took the same <a href="https://mkodama.org/content/EU-code/#a_brief_history_of_agi_companies_safety_commitments">commendable but insufficient steps</a> to manage risk that their American peers have taken. In particular, I want Chinese companies to do dangerous capability testing before deploying new frontier models and to follow published safety policies &#40;FSPs&#41;. The companies are not doing these things in the status quo. DeepSeek did <a href="https://thezvi.substack.com/i/180703503/reading-the-paper">no documented safety testing</a> whatsoever before they open-weighted v3.2.<sup id="fnref:DeepSeek"><a href="#fndef:DeepSeek" class=fnref >[3]</a></sup> Not one of the leading Chinese companies has published a safety policy.<sup id="fnref:SAIL"><a href="#fndef:SAIL" class=fnref >[4]</a></sup></p> <p>Now here&#39;s our intervention. We point out that FSPs are a reasonable way of implementing the CCP&#39;s stated policy goals on AI, and that China&#39;s government already has tools in place to mandate FSPs if it wishes to do so.</p> <p>Earlier this year, Xi Jinping announced that China should &quot;establish systems for technical monitoring, early risk warning and emergency response&quot; to guarantee AI&#39;s &quot;safety, reliability and controllability.&quot; Notice that Xi is talking about identifying risks in advance and taking steps to prevent safety incidents before they can strike. Even &quot;emergency response&quot; means something more than reaction in official Chinese thinking, also encompassing risk mitigation and early detection.<sup id="fnref:4phase"><a href="#fndef:4phase" class=fnref >[5]</a></sup> China&#39;s State Council, TC 260, and prominent Chinese academics have all echoed Xi&#39;s call for AI emergency preparedness. So the very highest levels of the Chinese state are calling for <em>proactive</em> AI risk management.</p> <p>What risks do they have in mind? There are some signs that catastrophic risks are on the CCP&#39;s agenda. Their 2025 National Emergency Response Plan listed AI security incidents in the same category as earthquakes and infectious disease epidemics. This suggests Chinese officials think AI could plausibly cause a mass casualty event soon. And moreover, they have in mind some of the same threat models that motivated Western RSPs. TC 260&#39;s <a href="https://www.cac.gov.cn/2025-09/15/c_1759653448369123.htm">AI Safety Governance Framework</a> explicitly mentioned WMD engineering uplift and rogue replication as key safety concerns.<sup id="fnref:TC260who"><a href="#fndef:TC260who" class=fnref >[6]</a></sup> Compare the two categories of dangerous capabilities covered by <a href="https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf">Anthropic&#39;s RSP</a>: CBRN weapons uplift and autonomous AI R&amp;D, which is concerning in part because it&#39;s a prerequisite for rogue replication.</p> <p>So one of China&#39;s stated goals is to proactively manage catastrophic risks from frontier AI. The good news for them is that there&#39;s a well-validated strategy for achieving this goal. You require every frontier AI company to publish an RSP, test new models for dangerous capabilities, and take the prescribed precautions if the tests reveal strong dangerous capabilities. California, New York, and the European Union have all agreed this is the way to go. All China has to do is copy their homework. </p> <p>Do Chinese regulators have the legal authority and operational capacity they&#39;d need to enforce a Chinese version of the EU Code of Practice? Sure they do. These regulators already make Chinese AI companies follow content security rules vastly more onerous and prescriptive than American or European catastrophic risk rules. The <a href="https://www.chinatalk.media/p/chinas-genai-content-security-standard">Basic Security Requirements for Gen AI Services</a> mandate thorough training data filtering and extensive predeployment testing, all to stop models from saying subversive things like &quot;May 35&quot; or &quot;Winnie the Pooh.&quot; If the CCP can make Chinese companies jump through all these hoops for censorship, it can absolutely make them write down FSPs and run some bio-uplift evals. </p> <p>For my part—and let me stress that I&#39;m speaking only for myself—I think making frontier AI companies write and follow Western-style FSPs would clearly be good from the CCP&#39;s perspective. The most obvious reason is that a global AI-induced catastrophe would hurt Chinese people and harm the interests of China&#39;s rulers, so the CCP should favor a cheap intervention to make such a catastrophe less likely. Another less direct benefit is that adopting global best-practices at home would make China&#39;s ongoing appeal for international cooperation on AI safety more credible. Li Qiang can make <a href="https://aisafetychina.substack.com/p/special-edition-world-ai-conference">all the speeches he wants</a> about China&#39;s commitment to safety. I don&#39;t expect US leaders to take this rhetoric seriously as long as all of China&#39;s frontier AI companies have <a href="https://futureoflife.org/ai-safety-index-summer-2025/">worse safety and transparency practices</a> than even xAI. But matters would be different if China passed binding domestic regulation at least as strong as SB 53. Such a signal of seriousness might help bring the US back to the negotiating table.</p> <p><br/> <table class=fndef  id="fndef:postercred"> <tr> <td class=fndef-backref ><a href="#fnref:postercred">[1]</a> <td class=fndef-content >Thanks to James for creating this poster. </table> <table class=fndef  id="fndef:B200s"> <tr> <td class=fndef-backref ><a href="#fnref:B200s">[2]</a> <td class=fndef-content >Especially if the US decides to <a href="https://ifp.org/should-the-us-sell-hopper-chips-to-china/">sell off much of our compute advantage</a> over China. </table> <table class=fndef  id="fndef:DeepSeek"> <tr> <td class=fndef-backref ><a href="#fnref:DeepSeek">[3]</a> <td class=fndef-content >At least one <a href="https://www.scmp.com/tech/article/3325742/deepseek-evaluates-ai-models-frontier-risks-source-says-china-promotes-safety">anonymous source has claimed</a> that DeepSeek does run dangerous capability evals before releasing a new model, and they just don&#39;t mention these evals to the outside world. I&#39;d give it less than a <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant=normal >/</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">1/5</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >1/5</span></span></span></span> chance that DeepSeek really does run SOTA dangerous capability evals internally, and even if they do, I have a problem with their lack of transparency. </table> <table class=fndef  id="fndef:SAIL"> <tr> <td class=fndef-backref ><a href="#fnref:SAIL">[4]</a> <td class=fndef-content >Notably, the Shanghai AI Laboratory has published a <a href="https://concordia-ai.com/research/frontier-ai-risk-management-framework/">detailed FSP</a> written in collaboration with Concordia. But I do not count SAIL as a frontier AI lab. </table> <table class=fndef  id="fndef:4phase"> <tr> <td class=fndef-backref ><a href="#fnref:4phase">[5]</a> <td class=fndef-content >The Emergency Response Law of the PRC does not, as one might naïvely expect, only cover what government should do once an emergency has already started. It also says how the Chinese government should prevent and prepare for emergencies, and how it should conduct surveillance to detect an active emergency as early as possible. </table> <table class=fndef  id="fndef:TC260who"> <tr> <td class=fndef-backref ><a href="#fnref:TC260who">[6]</a> <td class=fndef-content >For reference, TC 260 is the primary body responsible for setting cybersecurity and data protection standards in China. </table> </p> <div class=page-foot > <a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY-NC 4.0</a> Miles Kodama. Last modified: January 19, 2026. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </div> </div>