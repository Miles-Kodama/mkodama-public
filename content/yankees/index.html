<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <link rel=stylesheet  href="/css/custom.css"> <title>The Yankees Decision Problem</title> <body style="counter-reset: sidenote-counter;"> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/about/">About</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=the_yankees_decision_problem ><a href="#the_yankees_decision_problem" class=header-anchor >The <em>Yankees</em> Decision Problem</a></h1> <p>This scenario is supposed to counter the <em>Why ain&#39;cha rich?</em> argument for EDT by showing that causalists can predictably out-earn evidentialists when faced with the right sort of game. </p> <p>In Arntzenius&#39;s <a href="https://link.springer.com/article/10.1007/s10670-007-9084-8">original statement</a>, the Red Sox and the Yankees are going to play a long series of games against each other, and it&#39;s known that the Yankees have a 90&#37; chance of winning each game. The &#40;apparently ill-informed&#41; oddsmakers are offering two bets on the games: a bet that pays out &#36;1 if the Yankees win and loses &#36;2 if the Red Sox win, and a bet that pays out &#36;2 if the Red Sox win and loses &#36;1 if the Yankees win. You must choose one of these bets. The catch is that before you place your bet, the Sibyl will tell you whether or not you will win money on the bet. </p> <p>CDT bets on the Yankees every game because the Sybil&#39;s prediction has no causal bearing on the outcome of the game, and the Yankees bet is clearly underpriced. EDT bets on the Red Sox instead, no matter what the Sibyl says. This is because if you&#39;re guaranteed to win, you&#39;ll win twice as much by betting on the Sox, and if you&#39;re guaranteed to lose, you&#39;ll lose only half as much by betting on the Sox. One hundred games later, CDT is sitting on a &#36;70 expected gain while EDT is stinging from a &#36;70 expected loss. Then the causalist gets to throw the evidentialist&#39;s old taunt back at them: If you&#39;re so smart, why ain&#39;cha rich?</p> <p>I&#39;m uncertain whether this is a valid decision problem. <a href="https://philarchive.org/rec/AHMAOW">Ahmed and Price</a> object that the bettor in the <em>Yankees</em> scenario doesn&#39;t face a free choice under the Deliberation Crowds out Prediction &#40;DCOP&#41; thesis:</p> <blockquote> <p>The authority that an agent takes herself to have—qua agent—over her own future actions, seems inevitably to ‘trump’ whatever considerations might otherwise have formed the basis for a justified prediction &#40;probabilistic or otherwise&#41; about what she will choose to do.</p> </blockquote> <p>In other words, if you’re actually making a free choice, you can’t have reliable evidence about how you will choose, but the gambler in <em>Yankees</em> does have such evidence. If he knows that he will win his bet and he knows that the Yankees have a 90&#37; chance of winning, he must know something about his own probability of betting on the Yankees.</p> <p>As a reality check, we should ask why the DCOP thesis doesn&#39;t also render Newcomb&#39;s problem illegitimate. The answer is that even though Newcomb&#39;s Demon has information about what the chooser will do, that information is totally hidden from them &#40;inside an opaque box&#41; until after they’ve made their free choice. This defense doesn&#39;t seem to apply to the clear-box Newcomb problem, though. If I know that the predictor is 99&#37; accurate, and I walk into the room and see &#36;1M in the variable box, I now have very strong evidence that I will one-box. Ahmed and Price say in a footnote &#40;pg 22&#41; that they therefore consider the clear-box Newcomb problem as invalid as <em>Yankees</em>.</p> <p>I agree with Ahmed and Price that there&#39;s something fishy about the way the Sibyl is supposed to work in <em>Yankees</em>, but I don&#39;t think the problem is that her knowledge somehow &quot;crowds out&quot; your free choice. Rather, the problem is that the Sibyl can&#39;t give non-contradictory predictions to all agents. Suppose, for example, that you follow the policy of betting on the Red Sox if you are predicted to lose and betting on the Yankees if you are predicted to win. <a href="https://philpapers.org/archive/MEABAI.pdf">Meacham</a> points out that there&#39;s no way for the Sibyl to make her predictions consistently, because no matter what she tells you, the Red Sox have to win every game of an indefinitely long series to make her predictions come true. But this violates the assumption that the Red Sox have only a 10&#37; chance of winning each game. </p> <p>Now, it turns out that causalists and evidentialists all follow decision rules that admit of non-contradictory predictions because their gambling behavior doesn&#39;t change in response to the <em>content</em> of the Sibyl&#39;s predictions. Still, it seems like a serious defect of <em>Yankees</em> that the problem simply breaks when you pose it to some agents. Newcomb&#39;s problem does not suffer from a symmetric defect. You can follow any decision procedure you like, no matter how bizarre or trollish, and the problem will still be logically consistent. Compare this to the situation you face in <em>Yankees</em>: either you&#39;re the sort of agent who gambles the same no matter what the Sibyl tells you or else you&#39;ve been misled about the basic setup of the game you&#39;re playing. Seems highly suspicious.<sup id="fnref:Alex"><a href="#fndef:Alex" class=fnref >[1]</a></sup></p> <p>And even supposing that <em>Yankees</em> is a coherent problem, I still don&#39;t think it&#39;s an adequate rebuttal to <em>Why ain&#39;cha rich?</em> We care about the NP because it&#39;s analogous to decision scenarios that arise in real life non-trivially often. We often have to decide <a href="https://www.jstor.org/stable/2265034">whether to cooperate</a> under &#40;weak&#41; decision-entanglement, <a href="https://mindingourway.com/newcomblike-problems-are-the-norm/">whether to have integrity</a>, and so on. The <em>Why ain&#39;cha rich?</em> argument has bite because causalists actually will end up slightly poorer than evidentialists, even when they aren&#39;t playing contrived games against superhuman opponents. Do we face decision problems analogous to <em>Yankees</em> often enough to cancel this effect out? I rather doubt that we do.</p> <table class=fndef  id="fndef:Alex"> <tr> <td class=fndef-backref ><a href="#fnref:Alex">[1]</a> <td class=fndef-content >Thank you to Alex Kastner for talking through this with me. </table> <h2 id=what_does_cdt_actually_recommend ><a href="#what_does_cdt_actually_recommend" class=header-anchor >What does CDT actually recommend?</a></h2> <p>Arntzenius, Ahmed, Price, and Meacham all agree that causalists bet on the Yankees for the reason I gave above: the Sibyl&#39;s prediction is causally irrelevant to the outcome of the game, so CDT should ignore it. Caspar Oesterheld pointed out to me that there&#39;s something a bit funny about this argument. Suppose Mary the causal decision theorist reads Arntzenius&#39;s article and resolves to bet on the Yankees. Then the Sibyl tells her that she&#39;s going to lose her bet. If Mary has high confidence that she&#39;s going to bet on the Yankees and high confidence in the Sibyl&#39;s accuracy, she must have high confidence that the Red Sox are going to win. But then the action that will <em>cause</em> the best outcome is surely to bet on the Red Sox. And symmetrically, if Mary has high confidence that she&#39;s going to bet on the Red Sox, and the Sibyl predicts that she&#39;ll lose her bet, CDT recommends that she bet on the Yankees instead.</p> <p>The upshot is that when the bettor is predicted to lose, <em>Yankees</em> becomes a version of <em>Death in Damascus</em> where Death&#39;s victim has some small preference for one city over the other whether he lives or dies.<sup id="fnref:peterson"><a href="#fndef:peterson" class=fnref >[2]</a></sup> Evidentialists accept that they&#39;re equally likely to die no matter where they go and stay in Damascus to avoid the minor discomfort of journeying to Aleppo. Causalists succumb to decision instability and can&#39;t settle on <em>any</em> act unless randomization is available.</p> <table class=fndef  id="fndef:peterson"> <tr> <td class=fndef-backref ><a href="#fnref:peterson">[2]</a> <td class=fndef-content >See box 9.1 in Peterson&#39;s <a href="https://www.cambridge.org/highereducation/books/an-introduction-to-decision-theory/93E043CC210347D7F3FE39665FD26C9F#overview"><em>Intro to DT</em></a> </table> <h2 id=my_questions ><a href="#my_questions" class=header-anchor >My questions</a></h2> <ul> <li><p>Are there any common real life scenarios with the same structure as <em>Yankees</em>?</p> <li><p>If an evidentialist is given the chance to self-modify before facing <em>Yankees</em>, should they bind themselves to bet on the Yankees just as causalists should bind themselves to one-box?</p> <li><p>Can <em>Yankees</em> be salvaged if we declare by fiat that only pure EDT and CDT agents are allowed to play? Analogy: Newcomb&#39;s problem breaks if the agent&#39;s decision rule outputs an illegal move &#40;eg, take only the clear box&#41;, but this isn&#39;t a problem because such agents are banned from playing.</p> </ul> <div class=page-foot > <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> Miles Kodama. Last modified: August 05, 2025. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </div> </div>