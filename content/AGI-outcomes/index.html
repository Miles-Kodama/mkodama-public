<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <link rel=stylesheet  href="/css/custom.css"> <title>How Could Superintelligence Go for the Average Person?</title> <body style="counter-reset: sidenote-counter;"> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/about/">About</a> </ul> </div> <div id=main > <div class=franklin-content > <h1 id=how_could_superintelligence_go_for_the_average_person ><a href="#how_could_superintelligence_go_for_the_average_person" class=header-anchor >How Could Superintelligence Go for the Average Person?</a></h1> <p>The development of artificial general intelligence &#40;AGI&#41;, and superintelligence soon after, will be close to, if not <em>the</em> most significant event in history. The average person will likely have little to no effect on the trajectory of AI’s development. But its effect on them will be great. </p> <p>Philosophers and futurists who try to predict the shape of things to come often talk in grand terms about the <em>fate of the cosmic endowment</em>. But this zoomed-out perspective can sometimes miss what’s happening to the average person in the scenarios the philosophers debate. <strong>For someone like you or me, what could the future hold in store?</strong> Here’s our guess at the range of possible outcomes. The scenarios are ordered from worst to best in expectation, irrespective of how probable we think they are. </p> <p>&#40;1&#41; <strong>Malicious AI takeover.</strong> Extreme suffering. Superintelligence causes an <a href="https://longtermrisk.org/beginners-guide-to-reducing-s-risks/">s-risk</a>. See Scott Alexander&#39;s <a href="https://unsongbook.com/interlude-&#37;D7&#37;99-the-broadcast/">description of hell</a> in the book Unsong &#40;warning: graphic&#41; or Rational Animations’s video on <a href="https://www.youtube.com/watch?v&#61;fqnJcZiDMDo">fates worse than extinction</a>.</p> <p>&#40;2&#41; <strong>Extinction.</strong> The first superintelligence is misaligned. It views us the way we view insects—not with malice, but with cruel indifference. Our extinction might be a side effect of the superintelligence pursuing any number of <a href="https://ai-2027.com/research/ai-goals-forecast">goals</a>. It will probably be swift and mostly painless. There are other pathways too. AGI dramatically accelerates science, causing us to discover <a href="https://michaelnotebook.com/xriskbrief/index.html">recipes for ruin</a> faster than we can suppress or guard against them. Some malicious or incautious actor eventually cooks up one of the recipes, killing us all. We could also go extinct if great power conflict in the lead-up to AGI turns to thermonuclear war, or worse. </p> <p>&#40;3&#41; <strong>Malicious Human Takeover.</strong> A malevolent human dictator &#40;or malevolent oligarchy&#41; uses AGI to take over and create an indefinitely stable global regime. They punish their enemies. Unlike every other dictatorship in history, this regime is not limited by human administrative capacity. Stalin couldn&#39;t monitor every conversation in the Soviet Union, but with AI, he could have done. The dictator doesn&#39;t run the world well. The average person’s quality of life is dramatically lower than it was in the early 2020s.</p> <p>&#40;4&#41; <strong>Repugnant Conclusion.</strong> Many digital persons are created, but because of Malthusian trap dynamics, the marginal person’s standard of living is just high enough for them to be militarily/economically useful.<sup id="fnref:Hanson"><a href="#fndef:Hanson" class=fnref >[1]</a></sup> This need not be anyone&#39;s intention. Overpopulation just follows naturally from the ease of creating digital persons &#40;perhaps uploaded from original humans&#41;. If digital persons are allowed to replicate without limit, egalitarian systems will end up spreading scarce goods over a huge number of consumers. This dynamic can be regulated against, but <a href="https://nickbostrom.com/propositions.pdf">it won&#39;t be easy</a>.<sup id="fnref:Sam"><a href="#fndef:Sam" class=fnref >[2]</a></sup></p> <p>&#40;5&#41; <strong>Human Zoo.</strong> An autonomous AI system seizes absolute political power but does not kill everyone. If it preserves <em>some</em> humans for study—out of moral uncertainty, or for the sake of acausal trade with other AIs—the quality of life for those survivors will be high. The AI may take brain scans of the ones it does not choose to propagate through time. Interchangeable with the autonomous AI coup.</p> <p>&#40;6&#41; <a href="https://gradual-disempowerment.ai/"><strong>Gradual disempowerment</strong></a>. See also the <a href="https://intelligence-curse.ai/">intelligence curse</a>. The humans get slowly out-competed by AI in the economy of ideas, the literal economy, and the political arena. Automated corporations with AI workers soak up all demand. A full realization of Marx’s prediction: capital fully replaces labor, workers become expendable, and human wages are permanently driven to <a href="https://epoch.ai/gradient-updates/agi-could-drive-wages-below-subsistence-level">subsistence levels</a>.</p> <p>&#40;7&#41; <strong>No AGI.</strong> AGI never arrives. Ever.</p> <ul> <li><p>The governments of the world agree to collectively ban artificial superintelligence. They destroy all the advanced chip fabrication technology. Research on superintelligence becomes taboo within the scientific community, just as research on human germline gene editing is <a href="https://www.science.org/content/article/chinese-scientist-who-produced-genetically-altered-babies-sentenced-3-years-jail">taboo today</a>.<sup id="fnref:Dune"><a href="#fndef:Dune" class=fnref >[3]</a></sup></p> <li><p>The competitive pressures currently pushing toward AGI stop of their own accord because the economic benefits of AI are too hard to realize. Investment dries up. Another AI winter sets in, and this one never thaws. Moore&#39;s Law and <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">METR’s Law</a> stall indefinitely.</p> <li><p>We enter into a stable <a href="https://www.nationalsecurity.ai/">mutual deterrence stalemate</a>. Whenever a great power comes close to developing superintelligence, the other powers threaten it with force, and the renegade backs down.</p> </ul> <p>This outcome would be much better, if not for the fact that we are close on the tech tree to many scary technologies. The <a href="https://theprecipice.com/">baseline level of existential risk</a> is high. Preventing recipes for ruin from being discovered and misused might require intrusive AI-enabled civilizational restraint.<sup id="fnref:Adam"><a href="#fndef:Adam" class=fnref >[4]</a></sup></p> <p>&#40;8&#41; <strong>Benevolent Dictatorship.</strong> An enlightened human dictator or benevolent AI takes over. We get lucky, like Singapore did with Lee Kuan Yew. Whoever gets to steer the future cares about the wellbeing of the average human, and as such, the average person’s life improves along many dimensions. On the other hand, the vast majority of persons have little agency and next to no say over how the long-term future plays out.<sup id="fnref:Agency"><a href="#fndef:Agency" class=fnref >[5]</a></sup></p> <p>&#40;9&#41; <strong>Tool AIs.</strong> Capabilities keep rising, but AI remains more a tool or an oracle than an agent. We get the cure to cancer but no digital minds immediately. Society has enough time to adapt to technological change and to share the benefits of post-scarcity with all its members. The grand transhumanist vision is realized in the fullness of time, though it may be too late for this generation. Perhaps a pause on AI happened, or a concerted decision to pause to let the AIs do our alignment homework for us.</p> <p>&#40;10&#41; <strong>Long Reflection.</strong> The future comes quickly, but it turns out well. We get lucky. The intelligence explosion doesn&#39;t spin out of control. Our institutions adapt, and the superintelligences remain under human control. We learn how to use them safely as advisors, and they prevent us from doing anything <em>too</em> stupid with our technological superpowers—anything we would later regret. We are wise and humble. And we win.</p> <p><table class=fndef  id="fndef:Hanson"> <tr> <td class=fndef-backref ><a href="#fnref:Hanson">[1]</a> <td class=fndef-content >For an explanation of why societies of digital persons will tend toward the Malthusian equilibrium, see chapter 12 in Robin Hanson’s <a href="https://ageofem.com/">Age of Em</a>. </table> <table class=fndef  id="fndef:Sam"> <tr> <td class=fndef-backref ><a href="#fnref:Sam">[2]</a> <td class=fndef-content >Thanks to Samuel Ratnam and Adam Khoja. </table> <table class=fndef  id="fndef:Dune"> <tr> <td class=fndef-backref ><a href="#fnref:Dune">[3]</a> <td class=fndef-content >This also happens in the <em>Dune</em> novels after the <a href="https://en.wikipedia.org/wiki/Dune_&#40;franchise&#41;#The_Butlerian_Jihad">Butlerian Jihad</a> </table> <table class=fndef  id="fndef:Adam"> <tr> <td class=fndef-backref ><a href="#fnref:Adam">[4]</a> <td class=fndef-content >Thanks to Adam Khoja also for pointing this out. </table> <table class=fndef  id="fndef:Agency"> <tr> <td class=fndef-backref ><a href="#fnref:Agency">[5]</a> <td class=fndef-content >Under a benevolent dictatorship, humans could still enjoy a good deal of personal choice. But they would lack power and political choice. </table> <br/> <em>This article was co-written with <a href="https://jasonhausenloy.com/">Jason Hausenloy</a>. We thank <a href="https://www.samuelratnam.xyz/">Samuel Ratnam</a> and Adam Khoja for informative discussion and feedback.</em></p> <div class=page-foot > <a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY-NC 4.0</a> Miles Kodama. Last modified: November 01, 2025. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> </div> </div>